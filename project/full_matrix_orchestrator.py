import subprocess
import os
import json
import time
import glob
from datetime import datetime

# CONFIGURATION
GODOT_PATH = r"C:\Users\Usuario\Downloads\Godot_v4.5.1-stable_win64_console.exe"
PROJECT_PATH = r"C:\git\spellloop\project"
SCENE_PATH = r"scripts\debug\item_validation\TestRunner.tscn"
APPDATA_DIR = os.path.join(os.environ['APPDATA'], 'Godot', 'app_userdata', 'Spellloop', 'test_reports')
BATCH_SIZE = 25
TOTAL_ESTIMATE = 450 # Higher than 300 to be safe
TIMESTAMP = datetime.now().strftime("%Y-%m-%dT%H-%M-%S")
GLOBAL_FINAL_MD = os.path.join(PROJECT_PATH, f"global_balance_map_{TIMESTAMP}.md")

def run_batch(offset):
    cmd = [
        GODOT_PATH,
        "--headless",
        "--path", PROJECT_PATH,
        SCENE_PATH,
        "--",
        "--run-full",
        f"--batch-size={BATCH_SIZE}",
        f"--offset={offset}"
    ]
    print(f"\n>>> [Orchestrator] Running Batch: Offset {offset}, Size {BATCH_SIZE}...")
    
    # Use Popen to see live output if needed, but run is enough
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, errors='replace', timeout=400) # Increased timeout
        if result.returncode != 0:
            print(f"!!! [Orchestrator] CRASH or ERROR at offset {offset}")
            print(result.stdout[-1000:]) # Last 1000 chars
            print(result.stderr)
            return False, ""
        
        # Find the latest report generated by this batch
        # We look for the newest file because Godot saves it with a fresh timestamp
        list_of_files = glob.glob(os.path.join(APPDATA_DIR, "item_validation_report_*.jsonl"))
        if not list_of_files:
            return True, "" # Maybe no items found?
        
        latest_file = max(list_of_files, key=os.path.getctime)
        # Check if it was created in the last 2 minutes
        if (time.time() - os.path.getctime(latest_file)) > 120:
            return True, "" # No new report
            
        return True, latest_file
    except subprocess.TimeoutExpired:
        print(f"!!! [Orchestrator] TIMEOUT at offset {offset}")
        return False, ""

def aggregate_reports(report_files):
    print("\n>>> [Orchestrator] Aggregating %d reports..." % len(report_files))
    all_results = []
    for f in report_files:
        if not f: continue
        with open(f, 'r', encoding='utf-8', errors='replace') as file:
            for line in file:
                if line.strip():
                    all_results.append(json.loads(line))
    
    if not all_results:
        print("No results to aggregate.")
        return

    total = len(all_results)
    passed = 0
    violations = 0
    bugs = 0
    deltas = []
    
    for res in all_results:
        is_success = res.get("success", False)
        item_id = res.get("item_id", "unknown")
        
        subtests = res.get("subtests", [])
        has_violation = False
        has_bug = False
        
        for sub in subtests:
            res_data = sub.get("res", {})
            code = res_data.get("result_code", "PASS")
            if code == "BUG": has_bug = True
            elif code == "DESIGN_VIOLATION": has_violation = True
            
            if sub.get("type") == "mechanical_damage":
                delta = res_data.get("delta_percent", 0.0)
                deltas.append({
                    "id": item_id, 
                    "delta": delta, 
                    "actual": res_data.get("actual"), 
                    "expected": res_data.get("expected"),
                    "code": code
                })
        
        if has_bug: bugs += 1
        elif has_violation: violations += 1
        
        if is_success and not has_bug: # success is only false for bugs now
            passed += 1

    deltas.sort(key=lambda x: abs(x['delta']), reverse=True)
    top_20 = deltas[:20]

    with open(GLOBAL_FINAL_MD, 'w', encoding='utf-8') as f:
        f.write("# Global Balance Map\n")
        f.write(f"Generated: {datetime.now().isoformat()}\n\n")
        
        f.write("## Summary Metrics\n")
        f.write(f"- **Total Items**: {total}\n")
        f.write(f"- **Pass Rate**: {(passed/total*100):.1f}% ({passed})\n")
        f.write(f"- **Design Violation Rate**: {(violations/total*100):.1f}% ({violations})\n")
        f.write(f"- **Bug Rate**: {(bugs/total*100):.1f}% ({bugs})\n\n")
        
        f.write("## Top 20 Extreme Departures\n")
        f.write("| Item | Delta % | Actual | Expected | Class |\n")
        f.write("| :--- | :--- | :--- | :--- | :--- |\n")
        for d in top_20:
            f.write(f"| {d['id']} | {d['delta']*100:.1f}% | {d['actual']:.1f} | {d['expected']:.1f} | {d['code']} |\n")
            
        f.write("\n## Raw Failure Log\n")
        for res in all_results:
            if res.get("failures"):
                f.write(f"- **{res['item_id']}**: {res['failures']}\n")

    print(f"\n>>> [Orchestrator] Global report saved to: {GLOBAL_FINAL_MD}")

if __name__ == "__main__":
    found_reports = []
    for offset in range(0, TOTAL_ESTIMATE, BATCH_SIZE):
        success, report_file = run_batch(offset)
        if report_file:
            found_reports.append(report_file)
        
        # If a batch produced no report, we might be at the end
        if not report_file and offset > 0:
             print("No more items discovered. Stopping.")
             break
             
        if not success:
            print("Stopping due to failure.")
            break
            
        time.sleep(1)
    
    aggregate_reports(found_reports)
